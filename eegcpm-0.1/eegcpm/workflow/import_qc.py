"""QC metrics import from JSON files to workflow state database.

This module provides functions to import QC metrics JSON files (generated by CLI/HPC)
into the local workflow state database for UI display and tracking.
"""

import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from ..modules.qc.metrics_io import load_qc_metrics_json
from ..modules.qc.models import RunQualityMetricsFromJSON
from .state import ProcessingStatus, StepRecord, WorkflowState, WorkflowStateManager

logger = logging.getLogger(__name__)


def parse_qc_json_filename(filename: str) -> Optional[Dict[str, str]]:
    """
    Parse subject/session/task/run from QC JSON filename.

    Expected format: {subject}_ses-{session}_task-{task}_run-{run}_qc_metrics.json

    Parameters
    ----------
    filename : str
        Name of JSON file

    Returns
    -------
    dict or None
        Dict with subject_id, session, task, run keys, or None if parse fails
    """
    # Pattern: {subject}_ses-{session}_task-{task}_run-{run}_qc_metrics.json
    pattern = r'^(.+)_ses-(\d+)_task-([^_]+)_run-(\d+)_qc_metrics\.json$'
    match = re.match(pattern, filename)

    if match:
        return {
            'subject_id': match.group(1),
            'session': match.group(2),
            'task': match.group(3),
            'run': match.group(4)
        }

    return None


def scan_qc_json_files(
    derivatives_path: Path,
    subject_id: Optional[str] = None,
    task: Optional[str] = None,
    pipeline: Optional[str] = None
) -> List[Path]:
    """
    Scan derivatives directory for QC metrics JSON files.

    NEW: Updated for stage-first architecture.
    Scans: derivatives/preprocessing/{pipeline}/{subject}/ses-*/task-*/run-*/*_qc_metrics.json

    Parameters
    ----------
    derivatives_path : Path
        Path to derivatives directory
    subject_id : str, optional
        Filter by subject ID
    task : str, optional
        Filter by task name
    pipeline : str, optional
        Filter by pipeline name (preprocessing variant)

    Returns
    -------
    List[Path]
        List of paths to QC JSON files
    """
    derivatives_path = Path(derivatives_path)

    if not derivatives_path.exists():
        logger.warning(f"Derivatives path does not exist: {derivatives_path}")
        return []

    # NEW: Scan stage-first structure
    # derivatives/preprocessing/{pipeline}/{subject}/ses-*/task-*/run-*/*_qc_metrics.json
    preprocessing_dir = derivatives_path / "preprocessing"

    if not preprocessing_dir.exists():
        logger.warning(f"Preprocessing directory does not exist: {preprocessing_dir}")
        # Fallback: try old structure
        logger.info("Attempting to scan old directory structure...")
        json_files = list(derivatives_path.rglob("*_qc_metrics.json"))
    else:
        # Build pattern based on filters
        if pipeline:
            # Specific pipeline
            if subject_id:
                pattern = f"{pipeline}/sub-{subject_id}/ses-*/task-*/run-*/*_qc_metrics.json"
            else:
                pattern = f"{pipeline}/sub-*/ses-*/task-*/run-*/*_qc_metrics.json"
        else:
            # All pipelines
            if subject_id:
                pattern = f"*/sub-{subject_id}/ses-*/task-*/run-*/*_qc_metrics.json"
            else:
                pattern = f"*/sub-*/ses-*/task-*/run-*/*_qc_metrics.json"

        json_files = list(preprocessing_dir.glob(pattern))

    # Filter by subject/task if provided
    filtered = []
    for json_file in json_files:
        parsed = parse_qc_json_filename(json_file.name)
        if parsed is None:
            continue

        # Apply filters
        if subject_id:
            # Handle both with and without 'sub-' prefix
            subj = subject_id if subject_id.startswith('sub-') else f'sub-{subject_id}'
            parsed_subj = parsed['subject_id'] if parsed['subject_id'].startswith('sub-') else f"sub-{parsed['subject_id']}"
            if parsed_subj != subj:
                continue
        if task and parsed['task'] != task:
            continue

        filtered.append(json_file)

    return sorted(filtered)


def import_qc_metrics_to_state(
    derivatives_path: Path,
    state_manager: WorkflowStateManager,
    pipeline: str,
    task: Optional[str] = None,
    subject_id: Optional[str] = None,
    force: bool = False
) -> Dict[str, Any]:
    """
    Import QC metrics from JSON files into workflow state database.

    Scans the derivatives directory for *_qc_metrics.json files and creates
    corresponding workflow state entries in the database.

    Parameters
    ----------
    derivatives_path : Path
        Path to derivatives directory containing QC JSON files
    state_manager : WorkflowStateManager
        Workflow state manager instance
    pipeline : str
        Pipeline name to assign to imported workflows
    task : str, optional
        Filter imports to specific task
    subject_id : str, optional
        Filter imports to specific subject
    force : bool, default False
        If True, re-import even if workflow already exists in database

    Returns
    -------
    dict
        Summary with:
            - imported: int (number of runs imported)
            - skipped: int (number already in DB)
            - failed: int (number with errors)
            - subjects: list of subject IDs processed

    Examples
    --------
    >>> manager = WorkflowStateManager(db_path)
    >>> result = import_qc_metrics_to_state(
    ...     derivatives_path=Path("/data/derivatives"),
    ...     state_manager=manager,
    ...     pipeline="standard"
    ... )
    >>> print(f"Imported {result['imported']} runs")
    """
    derivatives_path = Path(derivatives_path)

    # Result tracking
    result = {
        'imported': 0,
        'skipped': 0,
        'failed': 0,
        'subjects': set(),
        'errors': []
    }

    # Scan for JSON files
    json_files = scan_qc_json_files(
        derivatives_path=derivatives_path,
        subject_id=subject_id,
        task=task,
        pipeline=pipeline  # NEW: Pass pipeline for stage-first scanning
    )

    logger.info(f"Found {len(json_files)} QC JSON files to import")

    for json_path in json_files:
        try:
            # Parse filename for metadata
            parsed = parse_qc_json_filename(json_path.name)
            if parsed is None:
                logger.warning(f"Could not parse filename: {json_path.name}")
                result['failed'] += 1
                result['errors'].append(f"Parse error: {json_path.name}")
                continue

            # Load JSON data
            try:
                json_data = load_qc_metrics_json(json_path)
            except Exception as e:
                logger.warning(f"Could not load JSON {json_path}: {e}")
                result['failed'] += 1
                result['errors'].append(f"Load error {json_path.name}: {str(e)}")
                continue

            # Parse into structured format
            try:
                metrics = RunQualityMetricsFromJSON.from_json(json_data)
            except Exception as e:
                logger.warning(f"Could not parse QC metrics from {json_path}: {e}")
                result['failed'] += 1
                result['errors'].append(f"Parse error {json_path.name}: {str(e)}")
                continue

            # Extract metadata
            subj = parsed['subject_id']
            sess = parsed['session']
            tsk = parsed['task']
            run = parsed['run']

            # Check if workflow already exists
            existing = state_manager.load_state(
                subject_id=subj,
                task=tsk,
                pipeline=pipeline,
                session=sess,
                run=run
            )

            if existing and not force:
                logger.debug(f"Skipping existing workflow: {subj}/{tsk}/run-{run}")
                result['skipped'] += 1
                continue

            # Get timestamp from JSON or use current time
            if metrics.timestamp:
                try:
                    created_at = datetime.fromisoformat(metrics.timestamp)
                except ValueError:
                    created_at = datetime.now()
            else:
                created_at = datetime.now()

            # Determine status based on quality metrics
            if metrics.quality_status in ('excellent', 'good', 'acceptable'):
                status = ProcessingStatus.COMPLETED
            elif metrics.quality_status == 'poor':
                status = ProcessingStatus.COMPLETED  # Still completed, just poor quality
            else:
                status = ProcessingStatus.COMPLETED

            # Create workflow state
            workflow = WorkflowState(
                subject_id=subj,
                task=tsk,
                pipeline=pipeline,
                status=status,
                session=sess,
                run=run,
                created_at=created_at
            )

            # Create step record with QC metrics
            step = StepRecord(
                step_name=f"preprocessing_run{run}",
                status=ProcessingStatus.COMPLETED,
                completed_at=created_at,
                output_path=str(json_path.parent),
                metadata={
                    'qc_metrics': json_data.get('quality_metrics', {}),
                    'imported_from_json': True,
                    'json_path': str(json_path),
                    'quality_status': metrics.quality_status,
                    'recommended_action': metrics.recommended_action,
                    'pct_bad_channels': metrics.pct_bad_channels,
                    'n_bad_channels': metrics.n_bad_channels,
                    'ica_success': metrics.ica_success,
                    'n_components_rejected': metrics.n_components_rejected,
                    'clustering_severity': metrics.clustering_severity
                }
            )

            workflow.add_step(step)

            # Save to database
            state_manager.save_state(workflow)

            result['imported'] += 1
            result['subjects'].add(subj)

            logger.debug(f"Imported: {subj}/{tsk}/run-{run}")

        except Exception as e:
            logger.error(f"Error processing {json_path}: {e}")
            result['failed'] += 1
            result['errors'].append(f"Error {json_path.name}: {str(e)}")

    # Convert subjects set to sorted list
    result['subjects'] = sorted(result['subjects'])

    logger.info(
        f"Import complete: {result['imported']} imported, "
        f"{result['skipped']} skipped, {result['failed']} failed"
    )

    return result


def get_import_summary(
    derivatives_path: Path,
    state_manager: WorkflowStateManager,
    pipeline: str
) -> Dict[str, Any]:
    """
    Get summary of what would be imported without actually importing.

    Parameters
    ----------
    derivatives_path : Path
        Path to derivatives directory
    state_manager : WorkflowStateManager
        Workflow state manager instance
    pipeline : str
        Pipeline name

    Returns
    -------
    dict
        Summary with:
            - total_json_files: int
            - new_to_import: int
            - already_in_db: int
            - subjects: list of unique subjects
            - tasks: list of unique tasks
    """
    derivatives_path = Path(derivatives_path)

    json_files = scan_qc_json_files(derivatives_path)

    subjects = set()
    tasks = set()
    new_count = 0
    existing_count = 0

    for json_path in json_files:
        parsed = parse_qc_json_filename(json_path.name)
        if parsed is None:
            continue

        subjects.add(parsed['subject_id'])
        tasks.add(parsed['task'])

        # Check if exists
        existing = state_manager.load_state(
            subject_id=parsed['subject_id'],
            task=parsed['task'],
            pipeline=pipeline,
            session=parsed['session'],
            run=parsed['run']
        )

        if existing:
            existing_count += 1
        else:
            new_count += 1

    return {
        'total_json_files': len(json_files),
        'new_to_import': new_count,
        'already_in_db': existing_count,
        'subjects': sorted(subjects),
        'tasks': sorted(tasks)
    }
